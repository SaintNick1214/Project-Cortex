# Cortex Python SDK Tests

Comprehensive test suite for Cortex SDK v0.11.0 with **actual data validation**.

## ğŸ¯ Test Philosophy

**Critical Principle**: All tests validate **actual data in databases**, not just "no errors".

Every test:

- âœ… Verifies data exists where expected
- âœ… Checks properties match requirements
- âœ… Validates relationships between entities
- âœ… Confirms metrics reflect reality

## ğŸ“ Test Structure

```
tests/
â”œâ”€â”€ streaming/              # Streaming API tests (70+ tests)
â”‚   â”œâ”€â”€ test_stream_metrics.py
â”‚   â”œâ”€â”€ test_stream_processor.py
â”‚   â”œâ”€â”€ test_chunking_strategies.py
â”‚   â”œâ”€â”€ test_progressive_storage.py
â”‚   â”œâ”€â”€ test_error_recovery.py
â”‚   â”œâ”€â”€ test_adaptive_processor.py
â”‚   â”œâ”€â”€ test_remember_stream_integration.py
â”‚   â”œâ”€â”€ manual_test.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ graph/                  # Graph integration tests
â”‚   â”œâ”€â”€ test_comprehensive_data_validation.py
â”‚   â”œâ”€â”€ comprehensive_validation.py  # Manual validation
â”‚   â””â”€â”€ clear_databases.py
â”‚
â”œâ”€â”€ conftest.py            # Shared fixtures
â””â”€â”€ run_streaming_tests.sh # Test runner
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Required
export CONVEX_URL="https://your-project.convex.cloud"
export NEO4J_URI="bolt://localhost:7687"
export NEO4J_USER="neo4j"
export NEO4J_PASSWORD="password"

# Optional (for Memgraph tests)
export MEMGRAPH_URI="bolt://localhost:7688"
export MEMGRAPH_USER=""
export MEMGRAPH_PASSWORD=""
```

### 2. Start Services

```bash
# From Project-Cortex root
docker-compose -f docker-compose.graph.yml up -d

# Start Convex dev
npx convex dev
```

### 3. Run Tests

```bash
# All streaming tests
./tests/run_streaming_tests.sh

# Or use pytest directly
python -m pytest tests/streaming/ -v

# Single test file
python -m pytest tests/streaming/test_stream_metrics.py -v

# With coverage
python -m pytest tests/streaming/ --cov=cortex.memory.streaming --cov-report=html
```

## ğŸ§ª Test Categories

### Unit Tests (50+ tests)

Fast tests with mocked dependencies:

- `test_stream_metrics.py` - 15 tests
- `test_chunking_strategies.py` - 10 tests
- `test_progressive_storage.py` - 8 tests
- `test_error_recovery.py` - 9 tests
- `test_adaptive_processor.py` - 9 tests

### Integration Tests (20+ tests)

Tests requiring live databases:

- `test_stream_processor.py` - 8 tests
- `test_remember_stream_integration.py` - 8 tests
- `test_comprehensive_data_validation.py` - 6 tests

### Manual Tests

Interactive tests with console output:

- `streaming/manual_test.py` - Full streaming demo
- `graph/comprehensive_validation.py` - Graph validation across all APIs
- `graph/clear_databases.py` - Database cleanup utility

## ğŸ“Š What Gets Validated

### Convex (Layer 1)

- âœ… Conversations exist with correct message counts
- âœ… Messages contain expected content
- âœ… Metadata is properly stored

### Vector (Layer 2)

- âœ… Memory entries exist with correct IDs
- âœ… Embeddings are generated when requested
- âœ… Conversation references are linked

### Facts (Layer 3)

- âœ… Fact records exist with proper structure
- âœ… Subject-predicate-object triples are correct
- âœ… Confidence scores match expectations

### Graph (External)

- âœ… Nodes exist with correct labels
- âœ… Properties match database records
- âœ… Edges connect the right nodes
- âœ… Traversal returns expected paths

### Streaming

- âœ… Metrics match actual stream processing
- âœ… Hooks receive correct events
- âœ… Progressive features update incrementally
- âœ… Error recovery strategies work as specified

## ğŸ” Running Specific Tests

### Run Only Fast Unit Tests

```bash
python -m pytest tests/streaming/ -m unit -v
```

### Run Only Integration Tests

```bash
python -m pytest tests/streaming/ -m integration -v
```

### Run Only Graph Tests

```bash
python -m pytest tests/streaming/ -m graph -v
```

### Run Single Test Method

```bash
python -m pytest tests/streaming/test_stream_metrics.py::TestMetricsCollector::test_metrics_initialization -v
```

## ğŸ› Debugging Failed Tests

### 1. Check Database Connectivity

```bash
# Neo4j
docker exec -it cortex-neo4j cypher-shell -u neo4j -p password

# Memgraph
docker exec -it cortex-memgraph mgconsole
```

### 2. Clear Test Data

```bash
python tests/graph/clear_databases.py
```

### 3. Verify Convex Backend

```bash
# Check Convex is running
npx convex dev

# View Convex dashboard
# Check for any errors in function logs
```

### 4. Run Manual Validation

```bash
# Comprehensive graph validation
python tests/graph/comprehensive_validation.py

# Streaming validation
python tests/streaming/manual_test.py
```

### 5. Enable Verbose Output

```bash
python -m pytest tests/streaming/test_stream_metrics.py -v -s --tb=long
```

## ğŸ“ˆ Test Coverage Goals

**Current Coverage**: ~70 tests

| Category         | Target | Current | Status  |
| ---------------- | ------ | ------- | ------- |
| Stream Metrics   | 15     | 15      | âœ… 100% |
| Stream Processor | 10     | 8       | âœ… 80%  |
| Chunking         | 10     | 10      | âœ… 100% |
| Storage          | 10     | 8       | âœ… 80%  |
| Error Recovery   | 10     | 9       | âœ… 90%  |
| Adaptive         | 10     | 9       | âœ… 90%  |
| Integration      | 15     | 8       | â³ 53%  |
| Graph Validation | 10     | 6       | â³ 60%  |

**To Reach 119 Tests**: Add more edge cases and error scenarios

## âœ… Test Quality Checklist

When writing tests, ensure:

- [ ] Test name describes what's being validated
- [ ] Docstring explains the test purpose
- [ ] CRITICAL comments mark key assertions
- [ ] Actual data is validated (not just no errors)
- [ ] Test cleans up after itself
- [ ] Error messages are descriptive

## ğŸ“ Example: Good vs Bad Test

### âŒ Bad Test (No Data Validation)

```python
async def test_agent_registration():
    # Just checks no error occurred
    await client.agents.register(agent_id="test", name="Test")
    assert True  # No validation!
```

### âœ… Good Test (Actual Data Validation)

```python
async def test_agent_registration_creates_graph_node():
    """Validates agent node exists in graph with correct properties"""
    agent_id = "test-agent"
    await client.agents.register(agent_id=agent_id, name="Test", sync_to_graph=True)

    # CRITICAL: Verify node actually exists
    nodes = await graph.find_nodes("Agent", {"agentId": agent_id}, 1)
    assert len(nodes) == 1, "Agent node not found!"
    assert nodes[0].properties["name"] == "Test"
```

## ğŸ“š Related Documentation

- [Streaming Tests README](streaming/README.md) - Detailed streaming test docs
- [TypeScript Tests](../../tests/) - Reference implementation
- [IMPLEMENTATION-STATUS.md](../IMPLEMENTATION-STATUS.md) - Current status

## ğŸš€ Next Steps

To expand test coverage:

1. Add more edge case tests for each component
2. Add performance benchmark tests
3. Add concurrency tests for streaming
4. Add cross-database consistency tests
5. Add failure scenario tests (network issues, database down, etc.)
