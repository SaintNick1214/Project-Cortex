---
id: semantic-search
title: Semantic Search
sidebar_position: 3
description: AI-powered memory retrieval using vector embeddings
---

# Semantic Search

<Callout type="info" title="What is Semantic Search?">
  Semantic search understands **meaning**, not just keywords. Search for "what's the user's favorite color?" and find memories about color preferences—even if they don't contain the exact words.
</Callout>

---

## Quick Start

```typescript
// Search with semantic understanding
const memories = await cortex.memory.search(
  "user-123-space",
  "what are the user's dietary preferences?",
  {
    embedding: await embed("what are the user's dietary preferences?"),
    userId: "user-123",
    limit: 5,
  }
);
```

---

## recall() vs search()

<ComparisonTable
  headers={["recall()", "search()"]}
  items={[
    { feature: "Primary Use", values: ["Unified orchestrated retrieval", "Vector-focused search"] },
    { feature: "Sources Queried", values: ["Vector + Facts + Graph", "Vector only"] },
    { feature: "Ranking", values: ["Multi-signal scoring", "Vector similarity only"] },
    { feature: "LLM Context", values: ["Built-in formatting", "Manual formatting"] },
    { feature: "Best For", values: ["RAG, context building", "Simple searches, performance"] },
  ]}
/>

<Tabs>
  <TabItem value="recall" label="recall() (Recommended)">

Unified retrieval across all memory layers:

```typescript
const result = await cortex.memory.recall({
  memorySpaceId: "user-123-space",
  query: "user preferences",
  embedding: await embed("user preferences"),
  userId: "user-123",
  limit: 10,
  sources: {
    vector: true,   // Search vector memories
    facts: true,    // Search structured facts
    graph: true,    // Query graph relationships
  },
  formatForLLM: true,
});

// Ready-to-use LLM context
console.log(result.context);

// Individual items with source info
result.items.forEach(item => {
  console.log(`${item.content} (${item.source}: ${item.score})`);
});
```

  </TabItem>
  <TabItem value="search" label="search()">

Direct vector search for performance-critical scenarios:

```typescript
const memories = await cortex.memory.search(
  "user-123-space",
  "user preferences",
  {
    embedding: await embed("user preferences"),
    userId: "user-123",
    limit: 10,
  }
);

memories.forEach(memory => {
  console.log(`${memory.content} (score: ${memory.score})`);
});
```

  </TabItem>
</Tabs>

---

## Search Strategies

<Tabs>
  <TabItem value="semantic" label="Semantic (Default)">

Uses vector embeddings for meaning-based search:

```typescript
const results = await cortex.memory.search(spaceId, query, {
  embedding: await embed(query),
  strategy: "semantic",
});
```

<Callout type="tip">
  Best for natural language queries and concept matching. "car" finds "automobile".
</Callout>

  </TabItem>
  <TabItem value="keyword" label="Keyword">

Traditional text matching (no embeddings required):

```typescript
const results = await cortex.memory.search(spaceId, "dark mode", {
  strategy: "keyword",
});
```

<Callout type="tip">
  Best for exact matches—names, IDs, specific terms. Works without embeddings.
</Callout>

  </TabItem>
  <TabItem value="auto" label="Auto">

Cortex chooses the best strategy automatically:

```typescript
// With embedding → tries semantic first
const results = await cortex.memory.search(spaceId, query, {
  embedding: await embed(query),
  strategy: "auto",  // Default
});

// Without embedding → uses keyword search
const results = await cortex.memory.search(spaceId, query);
```

**Fallback chain:** Semantic → Keyword → Recent memories

  </TabItem>
</Tabs>

---

## Embedding Providers

<Callout type="info">
  Cortex is **embedding-agnostic**. Use any provider that returns vectors.
</Callout>

<Tabs>
  <TabItem value="openai" label="OpenAI">

```typescript
import OpenAI from 'openai';

const openai = new OpenAI();

async function embed(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: text,
  });
  return response.data[0].embedding;
}
```

  </TabItem>
  <TabItem value="cohere" label="Cohere">

```typescript
import { CohereClient } from 'cohere-ai';

const cohere = new CohereClient();

async function embed(text: string): Promise<number[]> {
  const response = await cohere.embed({
    texts: [text],
    model: "embed-english-v3.0",
  });
  return response.embeddings[0];
}
```

  </TabItem>
  <TabItem value="vercel" label="Vercel AI SDK">

```typescript
import { embed } from 'ai';
import { openai } from '@ai-sdk/openai';

const { embedding } = await embed({
  model: openai.embedding('text-embedding-3-small'),
  value: text,
});
```

  </TabItem>
</Tabs>

### Embedding Model Comparison

<ComparisonTable
  headers={["Dimensions", "Speed", "Accuracy", "Cost"]}
  items={[
    { feature: "text-embedding-3-small (OpenAI)", values: ["1536", "Fast", "Good", "$"] },
    { feature: "text-embedding-3-large (OpenAI)", values: ["3072", "Medium", "Best", "$$"] },
    { feature: "embed-english-v3.0 (Cohere)", values: ["1024", "Fast", "Good", "$"] },
    { feature: "all-MiniLM-L6-v2 (Local)", values: ["384", "Very Fast", "Fair", "Free"] },
  ]}
/>

<Callout type="warning" title="Dimension Matching">
  Query embeddings must match stored embeddings. Don't mix 1536-dim queries with 3072-dim stored vectors.
</Callout>

---

## Search Options

<APITable parameters={[
  { name: "embedding", type: "number[]", required: false, description: "Query vector for semantic search" },
  { name: "strategy", type: "string", required: false, default: "auto", description: "'semantic', 'keyword', or 'auto'" },
  { name: "limit", type: "number", required: false, default: "20", description: "Maximum results to return" },
  { name: "minScore", type: "number", required: false, default: "0", description: "Minimum similarity score (0-1)" },
  { name: "userId", type: "string", required: false, description: "Filter by user" },
  { name: "tags", type: "string[]", required: false, description: "Filter by tags" },
  { name: "tagMatch", type: "string", required: false, default: "any", description: "'any' or 'all'" },
  { name: "minImportance", type: "number", required: false, description: "Minimum importance (0-100)" },
  { name: "createdAfter", type: "Date", required: false, description: "Filter by creation date" },
  { name: "createdBefore", type: "Date", required: false, description: "Filter by creation date" },
]} />

---

## Result Boosting

<Callout type="tip" title="Boost Options">
  Enhance ranking beyond pure semantic similarity:
  
```typescript
const results = await cortex.memory.search(spaceId, query, {
  embedding: await embed(query),
  boostImportance: true,  // Boost high-importance memories
  boostRecent: true,      // Boost recent memories
  boostPopular: true,     // Boost frequently accessed
});
```
</Callout>

---

## Filtering Examples

```typescript
// User-scoped search with tags
const memories = await cortex.memory.search(spaceId, query, {
  embedding: await embed(query),
  userId: "user-123",
  tags: ["preferences", "verified"],
  tagMatch: "all",  // Must have BOTH tags
});

// High-importance recent memories
const critical = await cortex.memory.search(spaceId, query, {
  embedding: await embed(query),
  minImportance: 80,
  createdAfter: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000),
  minScore: 0.75,
});

// Temporal search
const thisMonth = await cortex.memory.search(spaceId, query, {
  embedding: await embed(query),
  createdAfter: new Date("2026-01-01"),
  createdBefore: new Date("2026-01-31"),
});
```

---

## Search Result Format

```typescript
interface SearchResult {
  id: string;
  memorySpaceId: string;
  content: string;
  contentType: "raw" | "summarized";
  
  // Source info
  source: {
    type: "conversation" | "system" | "tool" | "a2a";
    userId?: string;
    userName?: string;
    timestamp: Date;
  };
  
  // Link to full conversation (ACID store)
  conversationRef?: {
    conversationId: string;
    messageIds: string[];
  };
  
  // Metadata
  metadata: {
    importance: number;  // 0-100
    tags: string[];
  };
  
  // Search-specific
  score: number;                              // 0-1
  strategy: "semantic" | "keyword" | "recent";
  highlights?: string[];
}
```

---

## Best Practices

<Callout type="tip" title="Always Provide Embeddings + User Scope">

```typescript
// Recommended: semantic search scoped to user
await cortex.memory.search(spaceId, query, {
  embedding: await embed(query),
  userId: userId,
});

// Works but less accurate (keyword fallback)
await cortex.memory.search(spaceId, query, {
  userId: userId,
});
```

</Callout>

<Callout type="tip" title="Use Descriptive Queries">

```typescript
// Vague - poor results
await cortex.memory.search(spaceId, "user");

// Specific - better results
await cortex.memory.search(spaceId, 
  "what are the user's dietary restrictions and food preferences?"
);
```

</Callout>

<Callout type="tip" title="Handle Empty Results">

```typescript
let memories = await cortex.memory.search(spaceId, query, {
  embedding: await embed(query),
  userId,
});

if (memories.length === 0) {
  // Broaden search: remove user filter
  memories = await cortex.memory.search(spaceId, query, {
    embedding: await embed(query),
    limit: 5,
  });
}

if (memories.length === 0) {
  // Final fallback: recent memories
  memories = await cortex.memory.search(spaceId, "*", {
    userId,
    sortBy: "createdAt",
    sortOrder: "desc",
    limit: 10,
  });
}
```

</Callout>

---

## Performance Tips

<Callout type="warning" title="Batch Embeddings">

```typescript
// Slow: generate embedding per query
for (const query of queries) {
  const embedding = await embed(query);
  await cortex.memory.search(spaceId, query, { embedding });
}

// Fast: batch embeddings first
const embeddings = await embedBatch(queries);
for (let i = 0; i < queries.length; i++) {
  await cortex.memory.search(spaceId, queries[i], { 
    embedding: embeddings[i] 
  });
}
```

</Callout>

<Callout type="tip" title="Filter During Search">

```typescript
// Slow: search everything, filter after
const all = await cortex.memory.search(spaceId, query, { limit: 100 });
const highPriority = all.filter(m => m.metadata.importance >= 80);

// Fast: filter during search
const highPriority = await cortex.memory.search(spaceId, query, {
  minImportance: 80,
  limit: 20,
});
```

</Callout>

---

## Common Use Cases

### Building LLM Context

```typescript
async function buildContext(spaceId: string, userId: string, message: string) {
  const memories = await cortex.memory.search(spaceId, message, {
    embedding: await embed(message),
    userId,
    minScore: 0.7,
    minImportance: 40,
    limit: 5,
  });

  if (memories.length === 0) {
    return "No prior context found.";
  }

  return `Relevant context:\n${memories
    .map(m => `- ${m.content}`)
    .join("\n")}`;
}
```

### Fact Retrieval

```typescript
async function getFact(spaceId: string, userId: string, question: string) {
  const memories = await cortex.memory.search(spaceId, question, {
    embedding: await embed(question),
    userId,
    limit: 1,
    minScore: 0.8,
  });

  return memories.length > 0 
    ? { fact: memories[0].content, confidence: memories[0].score }
    : { fact: null, confidence: 0 };
}
```

---

## Troubleshooting

<Accordion>
  <AccordionItem title="Search returns irrelevant results">
    
**Causes:** Query/content embedding mismatch, low score threshold

**Fix:**
```typescript
// Raise score threshold
const memories = await cortex.memory.search(spaceId, query, {
  embedding: await embed(query),
  minScore: 0.75,  // Raise from default
  tags: ["relevant-topic"],  // Narrow scope
});
```

  </AccordionItem>
  <AccordionItem title="No results found">
    
**Causes:** No matching memories, query too specific, wrong space ID

**Fix:**
```typescript
// Check if memories exist
const total = await cortex.memory.count(spaceId);
if (total === 0) {
  console.log("Memory space is empty");
}

// Try broader query
const broader = await cortex.memory.search(spaceId, "preferences", {
  embedding: await embed("preferences"),
  minScore: 0.5,
});
```

  </AccordionItem>
  <AccordionItem title="Search is slow">
    
**Causes:** Large result sets, complex filters, no caching

**Fix:**
```typescript
// Reduce scope with filters
const memories = await cortex.memory.search(spaceId, query, {
  embedding: await embed(query),
  createdAfter: new Date(Date.now() - 30 * 24 * 60 * 60 * 1000),
  limit: 10,
});

// Cache frequent queries
const cacheKey = `search:${spaceId}:${query}`;
const cached = cache.get(cacheKey);
if (cached) return cached;
```

  </AccordionItem>
</Accordion>

---

## Integration with Facts

Search automatically includes facts extracted via `remember()`:

```typescript
const result = await cortex.memory.recall({
  memorySpaceId: spaceId,
  query: "user preferences",
  embedding: await embed("user preferences"),
});

result.items.forEach(item => {
  if (item.fact) {
    console.log(`Fact: ${item.fact.fact}`);
    console.log(`Confidence: ${item.fact.confidence}%`);
    console.log(`Current belief: ${item.fact.isCurrentBelief}`);
  }
});
```

See [Fact Integration](/advanced-topics/fact-integration) for details on automatic fact extraction.

---

## Next Steps

<QuickNav>
  <QuickNavItem 
    title="Memory Operations API" 
    description="Complete search API reference" 
    href="/api-reference/memory-operations" 
  />
  <QuickNavItem 
    title="Search Strategy Architecture" 
    description="How search works internally" 
    href="/architecture/search-strategy" 
  />
  <QuickNavItem 
    title="Vector Embeddings" 
    description="Embedding models and dimensions" 
    href="/architecture/vector-embeddings" 
  />
  <QuickNavItem 
    title="Fact Integration" 
    description="Structured knowledge from conversations" 
    href="/advanced-topics/fact-integration" 
  />
</QuickNav>
